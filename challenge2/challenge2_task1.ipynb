{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqG/TIomANyznUQbeVEVfH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pJpBz1sVsJcq"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchsummary import summary\n","\n","import os\n","import copy\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt"],"metadata":{"id":"y1S59_qssK1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"metadata":{"id":"8oWPauFTsKy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root_dir = \"/gdrive/My Drive/CV/c2\"\n","\n","import sys\n","sys.path.append(root_dir)"],"metadata":{"id":"qdp6UdNpsKw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = root_dir + \"/recaptcha-dataset/Large\"\n","class_names = ['Bicycle', 'Bridge', 'Bus', 'Car', \n","               'Chimney', 'Crosswalk', 'Hydrant', \n","               'Motorcycle', 'Palm', 'Traffic Light']\n","\n","input_size = 224\n","batch_size = 32     # batch_size 는 본인이 적절하게 설정\n","\n","# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.RandomResizedCrop(input_size),   # data image resize\n","        transforms.RandomHorizontalFlip(),      # 좌우반전을 통해 데이터 증강 효과\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","image_datasets = datasets.ImageFolder(data_dir, data_transforms)  # your dataset / 데이터셋 다 읽어오는 코드\n","num_data = len(image_datasets)\n","indices = np.arange(num_data)\n","np.random.shuffle(indices)  # 데이터셋 랜덤으로 셔플\n","\n","train_size = int(num_data*0.8)\n","train_indices = indices[:train_size]    # 80% 인덱스는 훈련용 인덱스\n","val_indices = indices[train_size:]      # 20% 인덱스는 validation용 인덱스\n","train_set = torch.utils.data.Subset(image_datasets, train_indices)\n","val_set = torch.utils.data.Subset(image_datasets, val_indices)\n","\n","print('Number of training data:', len(train_set))\n","print('Number of validation data:', len(val_set))\n","\n","dataloaders = {'train': torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4), # num_workers 도 본인이 적절하게 설정\n","                 'val': torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4)}"],"metadata":{"id":"laWAQcWdsKun"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def imshow(imgs, title=None):\n","    \"\"\"Display image for Tensor.\"\"\"\n","    imgs = imgs.numpy().transpose((1, 2, 0))    # 1 : 길이, 2 : 너비\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    imgs = std * imgs + mean\n","    imgs = np.clip(imgs, 0, 1)\n","    plt.imshow(imgs)\n","    if title is not None:\n","        plt.title(title)"],"metadata":{"id":"ER2wsKyBsKsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a batch of training data\n","inputs, labels = next(iter(dataloaders['train']))\n","print(\"inputs.shape:\", inputs.shape)    # inputs.shape : [0] : batch, [1] : channel, [2]: 길이 , [3] : 너비\n","print(\"labels.shape:\", labels.shape)\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[:8])\n","\n","imshow(out, title=[class_names[x] for x in labels[:8]])"],"metadata":{"id":"QrQwXpN7sKqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a batch of validation data\n","inputs, labels = next(iter(dataloaders['val']))\n","print(\"inputs.shape:\", inputs.shape)\n","print(\"labels.shape:\", labels.shape)\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[:8])\n","\n","# imshow(out, title=[class_names[x] for x in labels[:8]])"],"metadata":{"id":"jJuv9DeTsKoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block(nn.Module):\n","    \n","    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n","        super(Block, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","        self.identity_downsample = identity_downsample\n","        \n","    def forward(self, x):\n","        identity = x\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        if self.identity_downsample is not None:\n","            identity = self.identity_downsample(identity)\n","        x += identity\n","        x = self.relu(x)\n","        return x"],"metadata":{"id":"QdsYlOfysKl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResNet_18(nn.Module):\n","    \n","    def __init__(self, image_channels, num_classes):\n","        \n","        super(ResNet_18, self).__init__()\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU()\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        \n","        #resnet layers\n","        self.layer1 = self.__make_layer(64, 64, stride=1)\n","        self.layer2 = self.__make_layer(64, 128, stride=2)\n","        self.layer3 = self.__make_layer(128, 256, stride=2)\n","        self.layer4 = self.__make_layer(256, 512, stride=2)\n","        \n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512, num_classes)\n","        \n","    def __make_layer(self, in_channels, out_channels, stride):\n","        \n","        identity_downsample = None\n","        if stride != 1:\n","            identity_downsample = self.identity_downsample(in_channels, out_channels)\n","            \n","        return nn.Sequential(\n","            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n","            Block(out_channels, out_channels)\n","        )\n","        \n","    def forward(self, x):\n","        \n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        \n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        \n","        x = self.avgpool(x)\n","        x = x.view(x.shape[0], -1)\n","        x = self.fc(x)\n","        return x \n","    \n","    def identity_downsample(self, in_channels, out_channels):\n","        \n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False), \n","            nn.BatchNorm2d(out_channels)\n","        )"],"metadata":{"id":"pnxund5_sKjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ResNet_18(image_channels=3, num_classes=10)\n","summary(model, (3, 224, 224), device='cpu')\n","# summary(model, (3, 512, 512), device='cpu')"],"metadata":{"id":"OWtfoanasKgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"metadata":{"id":"zGao2Bg6sYPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size"],"metadata":{"id":"lrBwa9K2sYM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","model_name = \"resnet\"\n","\n","num_classes = 10\n","num_epochs = 15\n","\n","# Flag for feature extracting. When False, we finetune the whole model,\n","#   when True we only update the reshaped layer params\n","feature_extract = False\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","summary(model_ft, (3, 224, 224), device='cpu')"],"metadata":{"id":"j7iB9hxJsYKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    model = model.to(device)\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"],"metadata":{"id":"8DGtBpY3sYH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"],"metadata":{"id":"FYDw7yNZsYFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Setup the loss fxn\n","# criterion = nn.CrossEntropyLoss()\n","\n","# # Train and evaluate\n","# model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)"],"metadata":{"id":"yEJYlX4FsYDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.save(model_ft, root_dir + '/resnet18_ft.pt')      # 그래서 이거 씀"],"metadata":{"id":"vS5BtJFAsYBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장한 모델 불러와서 사용\n","\n","model_ft = torch.load(root_dir + '/resnet18_ft.pt')\n","modules = list(model_ft.children())[:-1]\n","resnet18_feat = nn.Sequential(*modules)\n","for p in resnet18_feat.parameters():\n","    p.requires_grad = False\n","\n","for inputs, labels in dataloaders['val']:\n","    inputs = inputs.to(device)\n","    h = resnet18_feat(inputs)   # 히든 값의 특징값 \n","#     # print(h.shape)      # [32, 512, 1, 1] : 출력의 사이즈 [배치, 채널, 길이, 너비]\n","#     # 내가 해야하는건 512와 labels를 각각 대응해서 저장하는게 우리가 할 일\n","\n","#     '''\n","#     code:\n","#     save the (features, labels)\n","#     '''\n","    data_features = []\n","    data_labels = []\n","\n","    h = h.view(h.size(0), -1)\n","    data_features.extend(h.detach().cpu().numpy())\n","    data_labels.extend(labels.numpy())\n","\n","data_features = np.array(data_features)\n","data_labels = np.array(data_labels)\n"],"metadata":{"id":"PZJppponsX-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save data_features\n","\n","np.save(root_dir + '/data_features.npy', data_features)\n","np.save(root_dir + '/data_labels.npy', data_labels)"],"metadata":{"id":"7ThMZuNVsX8h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# task 1\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from joblib import dump, load\n","\n","# KNN classifier 객체 생성\n","knn = KNeighborsClassifier(n_neighbors=3)\n","\n","# Classifier 학습\n","knn.fit(data_features, data_labels)\n","\n","# 이후에 테스트 이미지의 피처를 추출하고 예측을 수행할 수 있습니다:\n","# test_features는 테스트 이미지의 피처입니다.\n","# test_preds = knn.predict(test_features)\n","\n","\n","\n","# knn 모델 저장\n","dump(knn, root_dir + '/knn_model_task1.joblib') \n","\n"],"metadata":{"id":"7-CGMcZ8slR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","from torchvision import transforms\n","\n","# 이미지 전처리를 위한 transform을 정의합니다.\n","transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Query 이미지 경로\n","query_dir = root_dir + '/query'\n","\n","# Query 폴더에서 이미지를 읽어옵니다.\n","query_images = os.listdir(query_dir)"],"metadata":{"id":"5V9MBQkpslPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","labels = ['Bicycle', 'Bridge', 'Bus', 'Car', 'Chimney', 'Crosswalk', 'Hydrant', 'Motorcycle', 'Palm', 'Traffic Light']\n","test_features = []\n","predict_labels = []\n","\n","for img_name in query_images:\n","    # 이미지를 불러옵니다.\n","    img_path = os.path.join(query_dir, img_name)\n","    img = Image.open(img_path).convert(\"RGB\")\n","\n","    # 전처리를 적용합니다.\n","    img = transform(img)\n","\n","    # 배치 차원을 추가합니다. (PyTorch 모델은 입력으로 4D 텐서를 기대합니다)\n","    img = img.unsqueeze(0).to(device)\n","\n","    # 이미지 피처를 뽑습니다.\n","    features = resnet18_feat(img)\n","\n","    # print(features)\n","\n","    # 평균 풀링을 적용합니다.\n","    features = F.adaptive_avg_pool2d(features, (1, 1))\n","\n","    # 피처를 1D로 flatten합니다.\n","    features = features.view(features.size(0), -1).cpu().detach().numpy()\n","\n","    # # KNN 모델을 불러옵니다.\n","    # predict_labels = knn.predict(features)\n","\n","    test_features.append(features)\n","    \n","# predict_labels = knn.predict(test_features[:2])\n","# print(labels[predict_labels])\n","\n","\n","\n","knn = load(root_dir + '/knn_model_task1_2.joblib')\n","\n","predict_labels = []\n","\n","for feature in test_features:\n","    predict_label = knn.predict(feature)\n","    predict_labels.append(labels[predict_label[0]])\n","\n","print(predict_labels)\n","\n"],"metadata":{"id":"9xuAHCT0slM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# task 1\n","\n","import csv\n","\n","with open(root_dir + '/c2_t1_a2.csv', 'w') as file:\n","    write = csv.writer(file)\n","    for i, predict_label in enumerate(predict_labels):\n","        write.writerow([f'query{i+1}.png', predict_label])"],"metadata":{"id":"2LSPzu_lslKN"},"execution_count":null,"outputs":[]}]}